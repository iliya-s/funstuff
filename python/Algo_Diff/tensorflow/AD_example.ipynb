{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Differentiation in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the analytic result for the evaluation and derivative of an arbitrary function. The first thing printed is the function evaluated at the chosen parameter values, and the second is the gradient with respect to each variable evaluated at those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4092974268256817\n",
      "1.4092974268256817 -0.6661468365471424\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Analytical result\n",
    "xa = 1.\n",
    "ya = 2.\n",
    "fa = xa * np.sin(ya) + (xa / ya)\n",
    "dfa_dxa = np.sin(ya) + 1 / ya\n",
    "dfa_dya = xa * np.cos(ya) - xa / (ya*ya)\n",
    "print(fa)\n",
    "print(dfa_dxa, dfa_dya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will do the same thing utilizing tensorflow. The outputs are in the same order as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40929743]\n",
      "[[ 1.40929743]\n",
      " [-0.66614684]]\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow example#\n",
    "#define tensorflow variables\n",
    "x = tf.placeholder(tf.float64)\n",
    "y = tf.placeholder(tf.float64)\n",
    "\n",
    "#define function\n",
    "f = x * tf.sin(y) + (x / y)\n",
    "\n",
    "#create gradient object\n",
    "grad = tf.gradients(f, [x, y])\n",
    "\n",
    "#create session\n",
    "s = tf.Session()\n",
    "val = s.run([f, grad], {x: [1], y: [2]} )\n",
    "s.close\n",
    "\n",
    "print(np.array(val[0]))\n",
    "print(np.array(val[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, this is an example of finding dx/dA for the equation b = Ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1301939 ]\n",
      " [0.16897504]\n",
      " [0.18005538]]\n",
      "[[[-0.02272082 -0.02948872 -0.03142241]\n",
      "  [-0.01009814 -0.0131061  -0.01396551]\n",
      "  [-0.00649166 -0.00842535 -0.00897783]]]\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow matrix example#\n",
    "#A*x = b, calculating derivative dx_dA(i,j) = sum_l dx(l)_dA(i,j)..... I think\n",
    "    #returns sum(dy/dx) for each independent variable, x\n",
    "#Build tensorflow objects\n",
    "A = tf.placeholder(dtype = tf.float32, shape=(3,3)) #A matrix of placeholder vals\n",
    "b = tf.constant([[1.0],[2.0],[3.0]])    #b vector\n",
    "#x = A^-1 * b\n",
    "x = tf.matmul(tf.matrix_inverse(A),b)\n",
    "\n",
    "grad = tf.gradients(x, A)\n",
    "\n",
    "s = tf.Session()\n",
    "val = s.run([x, grad], {A : [[5.0,1.0,1.0],[1.0,10.0,1.0],[1.0,1.0,15.0]]})\n",
    "s.close\n",
    "\n",
    "print(np.array(val[0]))\n",
    "print(np.array(val[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the output of the gradient is the sum(dy/dx) for each independent variable, x. I believe this was done because tensorflow was designed to perform back propogation and the full jacobian is not needed. Currently, tensorflow does not have an implementation for jacobians, but there are some workarounds you can find on stackexchange, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13019391]\n",
      " [0.16897507]\n",
      " [0.1800554 ]]\n",
      "[[[-0.02272082 -0.02948872 -0.03142241]\n",
      "  [-0.01009814 -0.0131061  -0.01396552]\n",
      "  [-0.00649166 -0.00842535 -0.00897783]]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(dtype=tf.float64, shape = (3,3))\n",
    "b = tf.constant([[1.0],[2.0],[3.0]],dtype=tf.float64)\n",
    "\n",
    "x0 = tf.random_uniform((3,1),dtype=tf.float64)\n",
    "r0 = b - tf.matmul(A,x0)\n",
    "p0 = r0\n",
    "dotr0 = tf.matmul(r0, r0, transpose_a = True)\n",
    "for i in range(30):\n",
    "    Ap = tf.matmul(A,p0)\n",
    "    pAp = tf.matmul(p0, Ap, transpose_a = True)\n",
    "    a = dotr0/pAp\n",
    "    x1 = x0 + tf.scalar_mul(tf.reshape(a,[]),p0)\n",
    "    r1 = r0 - tf.scalar_mul(tf.reshape(a,[]),Ap)\n",
    "    dotr1 = tf.matmul(r1, r1, transpose_a = True)\n",
    "    b = dotr1/dotr0\n",
    "    p1 = r1 + tf.scalar_mul(tf.reshape(b,[]),p0)\n",
    "    x0 = x1\n",
    "    r0 = r1\n",
    "    p0 = p1\n",
    "    dotr0 = dotr1\n",
    "\n",
    "grad = tf.gradients(x1, A)\n",
    "\n",
    "s = tf.Session()\n",
    "val = s.run([x1, grad], {A : [[5.0,1.0,1.0],[1.0,10.0,1.0],[1.0,1.0,15.0]]})\n",
    "s.close\n",
    "\n",
    "print(np.array(val[0]))\n",
    "print(np.array(val[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, this is an example using the conjugate gradient algorithm to solve a linear system of equations, and then find the gradient dx_dA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
